# [ì˜¤ëŠ˜ì˜ ë‚ ì”¨]
'''
íë¦¼, ì–´ì œë³´ë‹¤ OOÂ°C ë†’ì•„ìš”.
í˜„ì¬ OOÂ°C (ìµœì € OOÂ° / ìµœê³  OOÂ°)
ì˜¤ì „ ê°•ìˆ˜í™•ë¥  OO% / ì˜¤í›„ ê°•ìˆ˜í™•ë¥  OO%

ë¯¸ì„¸ë¨¼ì§€ OO ì¢‹ìŒ
ì´ˆë¯¸ì„¸ë¨¼ì§€ OO ì¢‹ìŒ 
'''
import re
import requests
from bs4 import BeautifulSoup


# bs4 ë¥¼ í†µí•´ ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ 
def create_soup(url) : 
  headers = {"User-Agent" : "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"}
  res = requests.get(url, headers=headers)
  res.raise_for_status()
  soup = BeautifulSoup(res.text, "lxml")
  return soup

# ë‰´ìŠ¤ë“¤ íƒ€ì´í‹€ ë° ë§í¬ ì¶œë ¥ í•¨ìˆ˜ 
def print_news(index, title, link) :
  print(f"{index+1}. {title}")
  print(f"  ğŸ“‚ {link}")

# ì´ë¯¸ì§€ í™•ì¸ í•¨ìˆ˜ 
def img_find(news) : 
  a_idx = 0 
  img = news.find("img")
  if img : 
    a_idx = 1 # img íƒœê·¸ê°€ ìˆìœ¼ë©´ 1ë²ˆì§¸ a íƒœê·¸ì˜ ì •ë³´ë¥¼ ì‚¬ìš©
  return a_idx

# ë‚ ì”¨ì •ë³´ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ 
def scrape_weather() : 
  print("[ì˜¤ëŠ˜ì˜ ë‚ ì”¨]")
  url = "https://search.naver.com/search.naver?sm=tab_hty.top&where=nexearch&query=%EC%84%9C%EC%9A%B8+%EB%82%A0%EC%94%A8&oquery=%EA%B2%BD%EA%B8%B0%EB%8F%84+%EC%9A%A9%EC%9D%B8%EC%8B%9C+%EA%B8%B0%ED%9D%A5%EA%B5%AC+%EA%B5%AC%EA%B0%88%EB%8F%99+%EB%AF%B8%EC%84%B8%EB%A8%BC%EC%A7%80&tqi=ig%2BwtwqVN8VssuOrpy0ssssss48-115725"
  soup = create_soup(url)
  
  # íë¦¼, ì–´ì œë³´ë‹¤ OOÂ°C ë†’ì•„ìš”. 
  cast = soup.find("p", attrs={"class" : "summary"}).get_text().split()
  # í˜„ì¬ OOÂ°C (ìµœì € OOÂ° / ìµœê³  OOÂ°) 
  curr_temp = soup.find("div", attrs={"class":"temperature_text"}).get_text().strip()  # ë§Œì•½ íƒœê·¸ë¥¼ ê°€ì ¸ì˜¬ ë•Œ í•´ë‹¹ íƒœê·¸ ì•ˆì— ë¶ˆí•„ìš”í•œ ë°ì´í„°ê°€ ìˆë‹¤ë©´, replace ë¥¼ ì¨ì„œ ë¹ˆì¹¸ìœ¼ë¡œ ë°”ê¿”ì£¼ì.
  min_temp = soup.find("span", attrs={"class": "lowest"}).get_text().strip()
  max_temp = soup.find("span", attrs={"class": "highest"}).get_text().strip()
  # ì˜¤ì „ ê°•ìˆ˜í™•ë¥  OO% / ì˜¤í›„ ê°•ìˆ˜í™•ë¥  OO%
  rain_rate = soup.find_all("span", attrs={"class":"weather_inner"})

  # ë¯¸ì„¸ë¨¼ì§€ 
  dust1 = soup.find_all("li",attrs={"class":"item_today level1"})


  # ì¶œë ¥ 
  print(f"{cast[0]} {cast[1]} {cast[2]}ğŸ’¡ ({cast[3]})")
  print("{} ({} / {})".format(curr_temp, min_temp, max_temp))
  print(f"ê°•ìˆ˜í™•ë¥  {rain_rate[0].get_text().strip()} / {rain_rate[1].get_text().strip()} ")
  print()
  print(f"{dust1[0].get_text().strip()}")
  print(f"{dust1[1].get_text().strip()}")
  print()



# í—¤ë“œë¼ì¸ ë‰´ìŠ¤ ì •ë³´ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def scrape_headline_news() : 
  print("[í—¤ë“œë¼ì¸ ë‰´ìŠ¤]")
  url = "https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=101"
  soup = create_soup(url)
  new_list = soup.find("ul", attrs={"class":"sh_list"}).find_all("li",limit=3)

  for index,news in enumerate(new_list):
    a_idx = img_find(news)
    title = news.find_all("a")[a_idx].get_text()
    link = news.find_all("a")[a_idx]["href"]
    print_news(index, title, link)
  print()
  

# IT ë‰´ìŠ¤ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def scrape_it_news() : 
  print("[IT ë‰´ìŠ¤]")
  url = "https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=105&sid2=230"
  soup = create_soup(url)
  news_list = soup.find("ul",attrs={"class":"type06_headline"}).find_all("li",limit=3)

  for index,news in enumerate(news_list) :
    a_idx = img_find(news) 
    a_tags = news.find_all("a")[a_idx]
    title = a_tags.get_text().strip()
    link = a_tags["href"]
    print_news(index, title, link)
  print()
    
# ì˜ì–´ íšŒí™” ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def scrape_english() : 
  print("[ì˜¤ëŠ˜ì˜ ì˜ì–´ íšŒí™”]")
  url = "https://www.hackers.co.kr/?c=s_eng/eng_contents/I_others_english&keywd=haceng_submain_lnb_eng_I_others_english&logger_kw=haceng_submain_lnb_eng_I_others_english#;"
  soup = create_soup(url)
  sentences = soup.find_all("div",attrs={"id":re.compile("^conv_kor_t")}) # conv_kor_t ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ìë¥¼ ë‹¤ ì°¾ëŠ” ì •ê·œì‹í‘œí˜„
  print("ğŸŠ ì˜ì–´ì§€ë¬¸")

  for sentence in sentences[len(sentences)//2:] : 
    print(sentence.get_text().strip())

  print()

  print("ğŸ¥­ í•œê¸€ì§€ë¬¸")

  for sentence in sentences[:len(sentences)//2] : 
    print(sentence.get_text().strip())
  
  



if __name__ == "__main__" : 
  scrape_weather() # ì˜¤ëŠ˜ì˜ ë‚ ì”¨ ì •ë³´ ê°€ì ¸ì˜¤ê¸° 
  scrape_headline_news() # í—¤ë“œë¼ì¸ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
  scrape_it_news() # IT ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
  scrape_english() # ì˜ì–´ íšŒí™” ì§€ë¬¸ ê°€ì ¸ì˜¤ê¸°
